# ğŸ§  LEO Core â€” Agents Specification (v0.2)

> Part of the **LEO Labs Open-Source Project**  
> Purpose: Define the modular LangGraph pipeline used to compute **AI Visibility Scores** for websites.

---

## âš™ï¸ Overview

**LEO Core** analyzes how easily Large Language Models (LLMs) can interpret and retrieve information from a website.

The system uses a **LangGraph agent pipeline** that processes input URLs, evaluates their structural and semantic clarity, computes a composite **LeoRank**, and generates AI-driven improvement suggestions.

### ğŸ§© Pipeline Diagram

```mermaid
graph LR
    A[CrawlerAgent] --> B[StructureAgent]
    B --> C[SemanticAgent]
    C --> D[ScoringAgent]
    D --> E[AdvisorAgent]
    E --> F[(Database)]
```

### ğŸ§® Scoring Formula

```
LeoRank = 100 * (0.4 * structure + 0.4 * semantic + 0.2 * retrieval)
```

---

## ğŸ§± Shared State (`LeoState`)

Each agent passes and mutates the shared `LeoState` object.

| Field | Type | Description |
|--------|------|-------------|
| `url` | `str` | Website URL being analyzed |
| `html` | `str` | Raw HTML fetched by the crawler |
| `text` | `str` | Cleaned readable text |
| `metrics` | `dict[str,float]` | Structural and semantic metrics |
| `leo_rank` | `float` | Final visibility score |
| `suggestions` | `list[str]` | AI-generated recommendations |

---

## ğŸ•¸ï¸ 1. CrawlerAgent

**Purpose:**  
Fetches and parses the target websiteâ€™s HTML.

**Logic:**
- Uses `requests` with headers & timeouts.
- Parses HTML via `BeautifulSoup`.
- Removes scripts, styles, and comments.
- Extracts readable text using heuristic filters.

**Outputs:**  
`state.html`, `state.text` populated.

**Metrics Affected:**  
None directly.

---

## ğŸ§± 2. StructureAgent

**Purpose:**  
Evaluate the HTML structure for clarity and metadata richness.

**Logic:**
- Counts `<meta>`, `<title>`, `<h1-h3>`, and `<schema.org>` tags.
- Checks for OpenGraph and Twitter card tags.
- Computes normalized score:
  ```
  structure_score = (meta_tags + og_tags + schema_tags) / normalization_factor
  ```

**Outputs:**  
`state.metrics["structure"]`

---

## ğŸ§  3. SemanticAgent

**Purpose:**  
Quantify semantic coherence and content accessibility for LLMs.

**Logic:**
- Splits text into paragraphs.
- Uses `openai.embeddings.create` to generate embeddings.
- Computes cosine similarity between consecutive embeddings.
- Produces `semantic_score = average_similarity`.

**LLM Calls:**  
Embeddings API (1 per 2â€“5 paragraphs, batched).

**Outputs:**  
`state.metrics["semantic"]`

---

## ğŸ”¢ 4. ScoringAgent

**Purpose:**  
Aggregate metrics and compute the final LeoRank.

**Logic:**
- Retrieves weights from `/leo/config/weights.yml`
- Applies formula:
  ```
  leo_rank = 100 * (0.4 * structure + 0.4 * semantic + 0.2 * retrieval)
  ```
- Saves result in DB via `save_score(url, leo_rank)`.

**Outputs:**  
`state.leo_rank`

---

## ğŸ’¬ 5. AdvisorAgent

**Purpose:**  
Provide optimization suggestions to improve LLM visibility.

**Logic:**
- If `OPENAI_API_KEY` available:
  - Calls GPT-4-mini with a concise prompt summarizing the siteâ€™s content and metrics.
  - Asks for actionable improvement steps.
- Else:
  - Generates static heuristic suggestions (e.g., â€œAdd schema.org metadataâ€).
- Appends output to `state.suggestions`.

**Outputs:**  
`state.suggestions` list populated.

---

## ğŸ—ƒï¸ Database Layer (Integration Agent)

**Purpose:**  
Persist results and enable analytics.

**Implementation:**  
SQLite by default, optional Postgres via Helm.

| Function | Description |
|-----------|--------------|
| `init_db()` | Creates tables if missing |
| `save_score(url, rank)` | Persists the score and timestamp |
| `get_recent_scores(limit)` | Fetches latest results |

---

## ğŸŒ MCP Server (Model Context Protocol)

**Purpose:**  
Expose LEO Core as a **GPT-native toolset** for external LLMs and agents.

**Endpoints:**
- `/tools/leo_audit?url=` â€” runs audit and returns report.
- `/tools/leo_recent` â€” retrieves recent scores.

**Manifest (`leo/mcp/config/manifest.json`):**
```json
{
  "schema_version": "1.0",
  "name": "leo-mcp",
  "description": "Run Leo audits and retrieve AI visibility insights.",
  "tools": {
    "leo_audit": {"input_schema": {"url": "string"}},
    "leo_recent": {"input_schema": {}}
  }
}
```

---

## â˜¸ï¸ Deployment Agents

**Helm Components:**
- `deployment.yaml` â€” FastAPI service  
- `mcp-deployment.yaml` â€” MCP microservice  
- `cronjob.yaml` â€” nightly re-audit task  
- `secret.yaml` â€” injects `OPENAI_API_KEY`  
- `postgres-dependency.yaml` â€” optional Postgres subchart  

**Homebrew Components:**
- `brew/leo-core.rb` â€” installation formula  
- `brew/postinstall.sh` â€” stores OpenAI API key  

---

## ğŸ” Testing and Validation

- `tests/test_pipeline.py` â€” verifies pipeline from crawl to rank.  
- `tests/test_api.py` â€” validates API endpoints.  
- `tests/test_agents.py` â€” ensures each agent modifies `LeoState` correctly.

---

## ğŸ§­ Future Extensions

| Planned | Description |
|----------|-------------|
| **Kafka Integration** | Stream audit logs to external analytics pipeline |
| **Rynions AI Agents** | Persona-based analysis advisors |
| **Dashboard UI** | Visual scoring leaderboard |
| **OpenSearch Export** | Searchable visibility dataset |

---

## ğŸ§¾ Version
**v0.2.0**  
Includes:  
- Full LangGraph agent logic  
- Database layer  
- MCP server integration  
- Helm + Brew deploy support  
- Auto-documentation (this file)

---

## ğŸ“œ License
Licensed under **MIT License** â€” open-source and free to modify for academic or commercial use.

---

### âœ… Maintainer Note
This `Agents.md` must remain synchronized with code versions.  
When adding or modifying agents, update this file to describe new logic and LLM interactions before merging into `main`.
